%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{enumerate} % Allows customizing enumeration blocks

\usepackage{qtree} % Render parse trees

\usepackage[noend]{algpseudocode}
\usepackage{algorithm} % create pseudocode blocks

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)


%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{MIT, 6.863 Natural Language Processing and Computer Representation of Knowledge}
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Automatic PCFG Generator \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author {Todd Layton, Anastasia Uryasheva}

\date{\normalsize\today} 

\begin{document}

\maketitle % 

%----------------------------------------------------------------------------------------
%	Part 1
%----------------------------------------------------------------------------------------

\section{Introduction}

Here will be an introduction

%----------------------------------------------------------------------------------------
%	Part 2
%----------------------------------------------------------------------------------------

\section{Background}

The theoretical background of the project is based on the article "Automatic learning of context-free grammar" written by T.Chen, C.Tseng, C.Chen. The article tells about the problem of learning context-free grammar from a corpus. The solution for the problem is investigated based on the notion of minimum description length of the corpus. 
Basic problem is to find a set of rules, which will derive original sentences in the best way. Article tells about the way of creating CFG from corpus of sentences and suggests a cost function as a measure of the quality of resulting CFG. With the help of cost function, goal is rewritten as: to find a set of rules that can derive original language with the minimum cost.

\subsection{Cost function}

Cost function depends on the CFG description of the corpus. There are two types of cost functions:

\begin{enumerate}[1.]
\item Rules for giving words their POS-tags.

For rules consisting of a non-terminal symbol on the left-hand side and a string of symbols on the right-hand side the cost of a rule is the number of bits needed to represent the left-hand side and right-hand side. For example, if we have the rule:
\begin{equation}\label{first}
A  -> \beta
\end{equation}
then the cost function for this rule will be:
\begin{equation}\label{first}
C_R = (1+\lvert\beta\rvert)log(\lvert\Sigma\rvert)
\end{equation}
where \begin{math} \Sigma \end{math}  is the symbol set and \begin{math} \lvert\Sigma\rvert \end{math} is the number of symbols in \begin{math} \Sigma \end{math} .
\item Rules for deriving sentences.

To derive sentence W we need a sequence of rules: \begin{math} S \Rightarrow \alpha_1 \Rightarrow ... \Rightarrow W \end{math}. This sequence of rules always starts with one of S-derivation rules: \begin{math} S \Rightarrow \alpha \end{math}. This step results in a derived string \begin{math} \alpha \end{math}. If there is no non-terminal symbols in \begin{math} \alpha \end{math} , we are done with the derivation. Otherwise, we expand the left-most non-terminal symbol, say X, in \begin{math} \alpha \end{math} by one of its derivation bodies. The process continues until there is no non-terminal symbols in the derived string, which will be the sentence W at that point.
So if we have a set of rules: R1(S) : S → … , … , R1(X) : X → … , … and a sentence for derivation W then the cost for rules to derive W will be:
\begin{equation}\label{first}
C_D = \sum \limits_{k=1}^m log(\lvert R(s_k)\rvert)
\end{equation}
where m is the number of rules in the derivation sequence, \begin{math} s_k \end{math} is the non-terminal symbol for the kth derivation and \begin{math} \lvert R(s_k)\rvert \end{math} is the number of rules in the CFG using \begin{math} s_k \end{math} as the left-hand side.
\end{enumerate}

Combining (2.2) and (2.3), the total cost is
\begin{equation}\label{first}
C = \sum \limits_{i=1}^p C_R(i) + \sum\limits_{j=1}^q C_D(j) = \sum \limits_{i=1}^p n_i log(\lvert \Sigma \rvert) + \sum\limits_{j=1}^q \sum\limits_{k=1}^{m_j} log(\lvert R(s_k) \rvert)
\end{equation}

\subsection{Special Cases}
Additionally, the article investigates two special cases:
\begin{enumerate}[1.]
\item Exhaustive CFG

Exhaustive CFG is a CFG that uses every distinct sentence in the corpus as a direct derivation body of the start symbol S. The number of symbols for a rule is simply the number of words of the corresponding sentense \begin{math} n_w \end{math}, plus 1 (for the start symbol S), and \begin{math} \lvert \Sigma \rvert \end{math} is the vocabulary size \begin{math} \lvert V \rvert \end{math} of the corpus plus 1 (for the start symbol). The rule cost is:
\begin{equation}\label{first}
C_R = n log(\lvert \Sigma \rvert) = (n_w + 1)log(\lvert V \rvert + 1)
\end{equation}
In this case, each sentence is derived from S in one step, by specifying the correct one out of the \begin{math} \lvert R(S) \rvert \end{math} rules. Thus the derivation cost for a sentence is:
\begin{equation}\label{first}
C_D = log \lvert R(S) \rvert
\end{equation}
 Total cost for that case is:

\begin{equation}\label{first}
C = \sum \limits_{i=1}^{\lvert R(S) \rvert} C_R(i) + \sum \limits_{j=1}^q C_D(j) = \sum \limits_{i=1}^{\lvert R(S) \rvert} (n_w(i) + 1) log(\lvert V \rvert + 1) + q log \lvert R(S) \rvert
\end{equation}

\item Recursive CFG

Recursive CFG is a CFG that uses recursive derivation for S: S → AS where the non-terminal A can be expanded to be any word in the vocabulary.
The rule cost in this case is:

\begin{equation}\label{first}
C_R = n log \lvert \Sigma \rvert
\end{equation}
where n can be 1, 2 or 3 depending on rule
The derivation cost in this case is:

\begin{equation}\label{first}
C_D = n_w(1 + log \lvert V \rvert) + 1
\end{equation}

Total cost in this case:

\begin{equation}\label{first}
C = \sum \limits_{i=1}^{2+\lvert V \rvert}n_i log \lvert \Sigma \rvert + \sum \limits_{j=1}^q C_D(j) = (4 + 2\lvert V \rvert)log(\lvert V \rvert + 2) + \sum \limits_{j=1}^q (n_w(j)(1 + log \lvert V \rvert) + 1)
\end{equation}

\end{enumerate}

The exhaustive CFG is too restricted in the sense that it covers only those sentences seen in the learning corpus. The recursive CFG is too broad in the sense that it covers all sentences including the non-sense ones. The goal is to strike a balance between these two extremes.


%----------------------------------------------------------------------------------------
%	Part 3
%----------------------------------------------------------------------------------------

\section{Implementation}

The \texttt{pcfggen} module's primary access point is the \texttt{generate\_pcfg()} function, which uses a POS-tagged training corpus to generate a PCFG through an iterative process of production rule creation and modification by transforming an initial trivial grammar into a more generalized one.

\begin{algorithm}
\caption{Pseudocode program flow of \texttt{pcfggen} module's \texttt{generate\_pcfg()} function}
\begin{algorithmic}
\Function{Generate-PCFG}{$S$} \Comment $S$: training corpus of POS-tagged sentences
\State $G = \text{Initial-Grammar($S$)}$
\Loop
\While{$G$ has valid rule joining $J$}
\State $G = \text{Join-Rules($G$, $J$)}$
\EndWhile
\If{$G$ has valid most-frequent 2-gram expansion $E$}
\State $G = \text{Expand-2-Gram($G$, $E$)}$
\Else
\State \Return $G$
\EndIf
\EndLoop
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Simplifying Assumptions}

This PCFG generator program make a couple basic assumptions to reduce the complexity of the problem, allowing it to focus more effectively on the core of the automated generation concept.

\subsubsection{Pre-Tagged Training Corpus}

Like many statistical parsers, the PCFGs generated by this program only describe production rules down to the level of parts of speech; the grammars' terminal symbols are not specific words, but rather POS tags. This means that the important training input to the generator is not a set of sentences in themselves, but a set of POS-tag sequences corresponding to a set of sentences. While this could be achieved by incorporating a separate tagging preparation step into the program itself, we chose to instead reduce the scope of the generator to exclude tagging; instead, the program's training corpus input is a set of POS-tagged sentences, which can have been tagged in any way the user sees fit.

\subsubsection{Flat-Probability Training Corpus}

The program calculates the relative frequency of various features in the grammar, as it is generated, on the assumption that each of the tagged sentences in the training corpus has the same flat probability in the source language (namely, a probability of 1 over the size-in-sentences of the corpus). While this reduces the amount of input data necessary, it also removed the possibility of the generation process taking into account the true relative probabilities of the training corpus's content, which could have a non-negligible impact on the resulting PCFG when training on larger corpora.

\subsection{Initial Grammar}

The program uses the input training corpus to create a simple grammar as a starting point for the rule modification. For each POS-tagged sentence in the training corpus, a corresponding production rule is added to the grammar. The left-hand side of this rule is \texttt{ROOT}, the right-hand side is the sequence of POS tags constituting that sentence, and the probability is $1$ over the total size, in sentences, of the corpus.

\subsection{Iteration}

Once the initial grammar is created, the program repeatedly looks for modifications to make in order to produce a grammar which is not so closely fitted to the training corpus. Generation of the PCFG is complete once there are no remaining modifications to be made. These modifications consist of two types of grammar transformations: 2-gram expansion and rule joining.

\subsubsection{2-gram Expansion}

The program finds the most frequently-occurring symbol 2-gram in the training corpus which is not already the right-hand side of a production rule in the grammar. A new nonterminal symbol is created, specified as an expansion of that 2-gram's symbols and with a unique numeric identifier, and each instance of that 2-gram is replaced with that nonterminal. An additional production rule is added to the grammar, of which the left-hand side is the new nonterminal, the right-hand side is the 2-gram, and the probability is $1$. This step does not change the functional structure of the grammar, but rather decomposed it into a larger number of simpler rules in order to enable rule joining.

\subsubsection{Rule Joining}

Before each 2-gram expansion step, the program checks whether any multiple production rules in the corpus, with more than one symbol in the right-hand side, consists of the exact same right-hand side except for the symbols at one particular index (for example, the third symbol in each rule's right-hand side). If such rules are found, then a new nonterminal symbol is created, specified as a rule joining of the symbols being replaced and with a unique numeric identifier, and the symbol at that index in each rule's right-hand side is replaced with that nonterminal. Then, for each of those rules, a new rule is added to the grammar, of which the left-hand side is the new nonterminal, the right-hand side is the symbol which that nonterminal replaced in that rule, and the probability is the relative frequency of the right-hand side symbol. Unlike 2-gram expansion, rule joining actually changes the functional structure of the grammar, allowing for symbol sequence which did not previously fit.

\subsection{Example}

The effects of these two types of grammar modification are best illustrated with a simple example. Consider a toy corpus of three sentences, corresponding to three POS tag sequences:

\begin{enumerate}[(a)]
\item \texttt{A C B C}
\item \texttt{X A C}
\item \texttt{Y B C}
\end{enumerate}

Given this corpus as the training input, the program would perform the following steps:
\begin{enumerate}[1.]
\item Create the initial grammar:
\begin{enumerate}[(a)]
\item \texttt{ROOT \textrightarrow A C B C [1/3]}
\item \texttt{ROOT \textrightarrow X A C [1/3]}
\item \texttt{ROOT \textrightarrow B C Y [1/3]}
\end{enumerate}

\item Check for rule joining, but no production rules are sufficiently similar.

\item Expand the 2-gram \texttt{A C}:
\begin{enumerate}[(a)]
\item \texttt{ROOT \textrightarrow exp(A)(C)-0 B C [1/3]}
\item \texttt{ROOT \textrightarrow X exp(A)(C)-0 [1/3]}
\item \texttt{ROOT \textrightarrow B C Y [1/3]}
\item \texttt{exp(A)(C)-0 \textrightarrow A C  [1]}
\end{enumerate}

\item Check for rule joining, but no production rules are sufficiently similar.

\item Expand the 2-gram \texttt{B C}:
\begin{enumerate}[(a)]
\item \texttt{ROOT \textrightarrow exp(A)(C)-0 exp(B)(C)-1 [1/3]}
\item \texttt{ROOT \textrightarrow X exp(A)(C)-0 [1/3]}
\item \texttt{ROOT \textrightarrow exp(B)(C)-0 Y [1/3]}
\item \texttt{exp(A)(C) \textrightarrow A C  [1]}
\item \texttt{exp(B)(C) \textrightarrow B C  [1]}
\end{enumerate}

\item Join \texttt{A C} and \texttt{B C}:
\begin{enumerate}[(a)]
\item \texttt{ROOT \textrightarrow exp(A)(C)-0 exp(B)(C)-1 [1/3]}
\item \texttt{ROOT \textrightarrow X exp(A)(C)-0 [1/3]}
\item \texttt{ROOT \textrightarrow exp(B)(C)-1 Y [1/3]}
\item \texttt{exp(A)(C)-0 \textrightarrow join(A)(B)-2 C  [1]}
\item \texttt{exp(B)(C)-1 \textrightarrow join(A)(B)-2 C  [1]}
\item \texttt{join(A)(B)-2 \textrightarrow A  [1/2]}
\item \texttt{join(A)(B)-2 \textrightarrow B  [1/2]}
\end{enumerate}

\item Check for rule joining, but no production rules are sufficiently similar.

\item Check for 2-gram expansion, but no unexpanded 2-grams remain.

\item Return the grammar.

\end{enumerate}

The resulting generated PCFG is able to parse not only the training sentences, but also other sentences of a similar structure. For example, the sentence \texttt{X B C} would be parsed as follows:

\Tree [.ROOT X [.exp(A)(C)-0 [.join(A)(B)-2 B ] C ]]

\subsection{Potential Improvements}

This implementation of a PCFG generation program could be improves significantly in several ways. At the moment, all valid 2-gram expansions and rule joinings which exists in the training corpus are executed, and the generation process only terminates once all such modifications have been completed. Allowing for the possibility of valid but statistically-negligible modification would improve the system's behavior on large training sets. Similarly, it is very possible for a generated grammar to include groups of production rules which could be simplified down to an equivalent form with fewer rules (as can be seen in the above example). This would limit the growth of the overall size of the generated PCFG's rule set, and might even create additional valid rule joinings of which the generation program could make use.

Another major improvement would be the incorporation of word and sentences probabilities into the training data. If the generation program could be run on raw sentences rather than on POS-tag sequences, and could account for the relative probabilities of each POS mapping to each word, then the generated grammar would be able to parse raw sentences directly, which would improve the accuracy of its results and massively increase its ability to generate natural-sounding random sentences. Similarly, incorporating information about the relative frequency of the training corpus's sentences would prevent less-grammatical input data from distorting the overall structure of the generated PCFG.

%----------------------------------------------------------------------------------------
%	Part 4
%----------------------------------------------------------------------------------------

\section{Results}

Here will be a table with results and results' description

%----------------------------------------------------------------------------------------
%	Part 5
%----------------------------------------------------------------------------------------
\section{Conclusion}

Here will be a conclusion

\end{document}

