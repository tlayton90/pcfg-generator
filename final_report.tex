%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{enumerate} % Allows customizing enumeration blocks

\usepackage{qtree} % Render parse trees

\usepackage[noend]{algpseudocode}
\usepackage{algorithm} % create pseudocode blocks

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)


%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{MIT, 6.863 Natural Language Processing and Computer Representation of Knowledge}
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Automatic PCFG Generator \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author {Todd Layton, Anastasia Uryasheva}

\date{\normalsize\today} 

\begin{document}

\maketitle % 

%----------------------------------------------------------------------------------------
%	Part 1
%----------------------------------------------------------------------------------------

\section{Introduction}

Here will be an introduction

%----------------------------------------------------------------------------------------
%	Part 2
%----------------------------------------------------------------------------------------

\section{Background}

The theoretical background of the project is based on the article "Automatic learning of context-free grammar" written by T.Chen, C.Tseng, C.Chen. The article tells about the problem of learning context-free grammar from a corpus. The solution for the problem is investigated based on the notion of minimum description length of the corpus. 
Basic problem is to find a set of rules, which will derive original sentences in the best way. Article tells about the way of creating CFG from corpus of sentences and suggests a cost function as a measure of the quality of resulting CFG. With the help of cost function, goal is rewritten as: to find a set of rules that can derive original language with the minimum cost.

\subsection{Cost function}

Cost function depends on the CFG description of the corpus. There are two types of cost functions:

\begin{enumerate}[1.]
\item Rules for giving words their POS-tags.

For rules consisting of a non-terminal symbol on the left-hand side and a string of symbols on the right-hand side the cost of a rule is the number of bits needed to represent the left-hand side and right-hand side. For example, if we have the rule:
\begin{equation}\label{first}
A  -> \beta
\end{equation}
then the cost function for this rule will be:
\begin{equation}\label{first}
C_R = (1+\lvert\beta\rvert)log(\lvert\Sigma\rvert)
\end{equation}
where \begin{math} \Sigma \end{math}  is the symbol set and \begin{math} \lvert\Sigma\rvert \end{math} is the number of symbols in \begin{math} \Sigma \end{math} .
\item Rules for deriving sentences.

To derive sentence W we need a sequence of rules: \begin{math} S \Rightarrow \alpha_1 \Rightarrow ... \Rightarrow W \end{math}. This sequence of rules always starts with one of S-derivation rules: \begin{math} S \Rightarrow \alpha \end{math}. This step results in a derived string \begin{math} \alpha \end{math}. If there is no non-terminal symbols in \begin{math} \alpha \end{math} , we are done with the derivation. Otherwise, we expand the left-most non-terminal symbol, say X, in \begin{math} \alpha \end{math} by one of its derivation bodies. The process continues until there is no non-terminal symbols in the derived string, which will be the sentence W at that point.
So if we have a set of rules: R1(S) : S → … , … , R1(X) : X → … , … and a sentence for derivation W then the cost for rules to derive W will be:
\begin{equation}\label{first}
C_D = \sum \limits_{k=1}^m log(\lvert R(s_k)\rvert)
\end{equation}
where m is the number of rules in the derivation sequence, \begin{math} s_k \end{math} is the non-terminal symbol for the kth derivation and \begin{math} \lvert R(s_k)\rvert \end{math} is the number of rules in the CFG using \begin{math} s_k \end{math} as the left-hand side.
\end{enumerate}

Combining (2.2) and (2.3), the total cost is
\begin{equation}\label{first}
C = \sum \limits_{i=1}^p C_R(i) + \sum\limits_{j=1}^q C_D(j) = \sum \limits_{i=1}^p n_i log(\lvert \Sigma \rvert) + \sum\limits_{j=1}^q \sum\limits_{k=1}^{m_j} log(\lvert R(s_k) \rvert)
\end{equation}

\subsection{Special Cases}
Additionally, the article investigates two special cases:
\begin{enumerate}[1.]
\item Exhaustive CFG

Exhaustive CFG is a CFG that uses every distinct sentence in the corpus as a direct derivation body of the start symbol S. The number of symbols for a rule is simply the number of words of the corresponding sentense \begin{math} n_w \end{math}, plus 1 (for the start symbol S), and \begin{math} \lvert \Sigma \rvert \end{math} is the vocabulary size \begin{math} \lvert V \rvert \end{math} of the corpus plus 1 (for the start symbol). The rule cost is:
\begin{equation}\label{first}
C_R = n log(\lvert \Sigma \rvert) = (n_w + 1)log(\lvert V \rvert + 1)
\end{equation}
In this case, each sentence is derived from S in one step, by specifying the correct one out of the \begin{math} \lvert R(S) \rvert \end{math} rules. Thus the derivation cost for a sentence is:
\begin{equation}\label{first}
C_D = log \lvert R(S) \rvert
\end{equation}
 Total cost for that case is:

\begin{equation}\label{first}
C = \sum \limits_{i=1}^{\lvert R(S) \rvert} C_R(i) + \sum \limits_{j=1}^q C_D(j) = \sum \limits_{i=1}^{\lvert R(S) \rvert} (n_w(i) + 1) log(\lvert V \rvert + 1) + q log \lvert R(S) \rvert
\end{equation}

\item Recursive CFG

Recursive CFG is a CFG that uses recursive derivation for S: S → AS where the non-terminal A can be expanded to be any word in the vocabulary.
The rule cost in this case is:

\begin{equation}\label{first}
C_R = n log \lvert \Sigma \rvert
\end{equation}
where n can be 1, 2 or 3 depending on rule
The derivation cost in this case is:

\begin{equation}\label{first}
C_D = n_w(1 + log \lvert V \rvert) + 1
\end{equation}

Total cost in this case:

\begin{equation}\label{first}
C = \sum \limits_{i=1}^{2+\lvert V \rvert}n_i log \lvert \Sigma \rvert + \sum \limits_{j=1}^q C_D(j) = (4 + 2\lvert V \rvert)log(\lvert V \rvert + 2) + \sum \limits_{j=1}^q (n_w(j)(1 + log \lvert V \rvert) + 1)
\end{equation}

\end{enumerate}

The exhaustive CFG is too restricted in the sense that it covers only those sentences seen in the learning corpus. The recursive CFG is too broad in the sense that it covers all sentences including the non-sense ones. The goal is to strike a balance between these two extremes.

\subsection{Difference between the theory and practice}
Although general theoretical background is described in this article the algorythm of PCFG generator has a couple of significant differences. The major is that article describes the generating set CFG rules at the time we're generating the list of PCFG rules which a way more difficult. That's why authors of the article had a strong view of a way to measure the quality of the processed grammar through the cost function - they want to minimize the size of grammar. We in contrast can't be so precise in determing the measure of quality. Instead of that we're making a goal of generalizing language structure from the training corpus. That's why we decided to take as a quality measure a difference between golden standard probability and probability which was provided by viterbi trained on our PCFG. Generally a higher average probability is better, though it's important to note that it's trivial to make a PCFG which always gives a parse of probability 1.

%----------------------------------------------------------------------------------------
%	Part 3
%----------------------------------------------------------------------------------------

\section{Design}

This program uses a POS-tagged training corpus to generate a PCFG, through an iterative process of production rule creation and modification by transforming an initial trivial grammar into a more generalized one.

\subsection{Simplifying Assumptions}

This PCFG generator program make a couple basic assumptions to reduce the complexity of the problem, allowing it to focus more effectively on the core of the automated generation concept.

\subsubsection{Pre-Tagged Training Corpus}

Like many statistical parsers, the PCFGs generated by this program only describe production rules down to the level of parts of speech; the grammars' terminal symbols are not specific words, but rather POS tags. This means that the important training input to the generator is not a set of sentences in themselves, but a set of POS-tag sequences corresponding to a set of sentences. While this could be achieved by incorporating a separate tagging preparation step into the program itself, we chose to instead reduce the scope of the generator to exclude tagging; instead, the program's training corpus input is a set of POS-tagged sentences, which can have been tagged in any way the user sees fit.

\subsubsection{Flat-Probability Training Corpus}

The program calculates the relative frequency of various features in the grammar, as it is generated, on the assumption that each of the tagged sentences in the training corpus has the same flat probability in the source language (namely, a probability of 1 over the size-in-sentences of the corpus). While this reduces the amount of input data necessary, it also removed the possibility of the generation process taking into account the true relative probabilities of the training corpus's content, which could have a non-negligible impact on the resulting PCFG when training on larger corpora.

\subsection{Initial Grammar}

The program uses the input training corpus to create a exhaustive grammar as a starting point for the rule modification. For each POS-tagged sentence in the training corpus, a corresponding production rule is added to the grammar. The left-hand side of this rule is \texttt{ROOT}, the right-hand side is the sequence of POS tags constituting that sentence, and the probability is $1$ over the total size, in sentences, of the corpus.
\subsection{Iteration}

As described above, exhaustive grammars are maximally specific to their generating corpus, so once the initial grammar is created, the program repeatedly looks for modifications to make in order to produce a grammar which is not so closely fitted to the training corpus. Generation of the PCFG is complete once there are no remaining modifications to be made. These modifications consist of two types of grammar transformations: 2-gram expansion and rule joining.

\subsubsection{2-gram Expansion}

The program finds the most frequently-occurring symbol 2-gram in the training corpus which is not already the right-hand side of a production rule in the grammar. A new nonterminal symbol is created, specified as an expansion of that 2-gram's symbols and with a unique numeric identifier, and each instance of that 2-gram is replaced with that nonterminal. An additional production rule is added to the grammar, of which the left-hand side is the new nonterminal, the right-hand side is the 2-gram, and the probability is $1$. This step does not change the functional structure of the grammar, but rather decomposed it into a larger number of simpler rules in order to enable rule joining.

\subsubsection{Rule Joining}

Before each 2-gram expansion step, the program checks whether any multiple production rules in the corpus, with more than one symbol in the right-hand side, consists of the exact same right-hand side except for the symbols at one particular index (for example, the third symbol in each rule's right-hand side). If such rules are found, then a new nonterminal symbol is created, specified as a rule joining of the symbols being replaced and with a unique numeric identifier, and the symbol at that index in each rule's right-hand side is replaced with that nonterminal. Then, for each of those rules, a new rule is added to the grammar, of which the left-hand side is the new nonterminal, the right-hand side is the symbol which that nonterminal replaced in that rule, and the probability is the relative frequency of the right-hand side symbol. Unlike 2-gram expansion, rule joining actually changes the functional structure of the grammar, allowing for symbol sequence which did not previously fit.

%----------------------------------------------------------------------------------------
%	Part 4
%----------------------------------------------------------------------------------------

\section{Implementation}

The \texttt{pcfggen} module's primary access point is the \texttt{generate\_pcfg()} function.

\subsection{Control Flow}

\begin{algorithm}
\caption{Pseudocode program flow of \texttt{pcfggen} module's \texttt{generate\_pcfg()} function}
\begin{algorithmic}
\Function{Generate-PCFG}{$S$} \Comment $S$: training corpus of POS-tagged sentences
\State $G = \text{Initial-Grammar($S$)}$
\Loop
\While{$G$ has valid rule joining $J$}
\State $G = \text{Join-Rules($G$, $J$)}$
\EndWhile
\If{$G$ has valid most-frequent 2-gram expansion $E$}
\State $G = \text{Expand-2-Gram($G$, $E$)}$
\Else
\State \Return $G$
\EndIf
\EndLoop
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Example}

The iterative results of this function are best illustrated with a simple example. Consider a toy corpus of three sentences, corresponding to three POS tag sequences:

\begin{enumerate}[(a)]
\item \texttt{A C B C}
\item \texttt{X A C}
\item \texttt{Y B C}
\end{enumerate}

Given this corpus as the training input, the program would perform the following steps:
\begin{enumerate}[1.]
\item Create the initial grammar:
\begin{enumerate}[(a)]
\item \texttt{ROOT \textrightarrow A C B C [1/3]}
\item \texttt{ROOT \textrightarrow X A C [1/3]}
\item \texttt{ROOT \textrightarrow B C Y [1/3]}
\end{enumerate}

\item Check for rule joining, but no production rules are sufficiently similar.

\item Expand the 2-gram \texttt{A C}:
\begin{enumerate}[(a)]
\item \texttt{ROOT \textrightarrow exp(A)(C)-0 B C [1/3]}
\item \texttt{ROOT \textrightarrow X exp(A)(C)-0 [1/3]}
\item \texttt{ROOT \textrightarrow B C Y [1/3]}
\item \texttt{exp(A)(C)-0 \textrightarrow A C  [1]}
\end{enumerate}

\item Check for rule joining, but no production rules are sufficiently similar.

\item Expand the 2-gram \texttt{B C}:
\begin{enumerate}[(a)]
\item \texttt{ROOT \textrightarrow exp(A)(C)-0 exp(B)(C)-1 [1/3]}
\item \texttt{ROOT \textrightarrow X exp(A)(C)-0 [1/3]}
\item \texttt{ROOT \textrightarrow exp(B)(C)-0 Y [1/3]}
\item \texttt{exp(A)(C) \textrightarrow A C  [1]}
\item \texttt{exp(B)(C) \textrightarrow B C  [1]}
\end{enumerate}

\item Join \texttt{A C} and \texttt{B C}:
\begin{enumerate}[(a)]
\item \texttt{ROOT \textrightarrow exp(A)(C)-0 exp(B)(C)-1 [1/3]}
\item \texttt{ROOT \textrightarrow X exp(A)(C)-0 [1/3]}
\item \texttt{ROOT \textrightarrow exp(B)(C)-1 Y [1/3]}
\item \texttt{exp(A)(C)-0 \textrightarrow join(A)(B)-2 C  [1]}
\item \texttt{exp(B)(C)-1 \textrightarrow join(A)(B)-2 C  [1]}
\item \texttt{join(A)(B)-2 \textrightarrow A  [1/2]}
\item \texttt{join(A)(B)-2 \textrightarrow B  [1/2]}
\end{enumerate}

\item Check for rule joining, but no production rules are sufficiently similar.

\item Check for 2-gram expansion, but no unexpanded 2-grams remain.

\item Return the grammar.

\end{enumerate}

The resulting generated PCFG is able to parse not only the training sentences, but also other sentences of a similar structure. For example, the sentence \texttt{X B C} would be parsed as follows:

\Tree [.{ROOT[ $1 \over 3$]} X [.{exp(A)(C)-0[$1$]} [.{join(A)(B)-2[$1 \over 2$]} B ] C ]]

Similarly, the generated grammar is capable of randomly producing sentences which were not present in the training corpus. \texttt{X B C} is a possible random-sentence output of this PCFG, with a probability of ${1 \over 3} \cdot 1 \cdot {1 \over 2} = {1 \over 6}$.

%----------------------------------------------------------------------------------------
%	Part 5
%----------------------------------------------------------------------------------------

\section{Results}

To check the quality of generated PCFG we decided to parse every sentence by \texttt{viterbi parser} which is a part of \texttt{nltk.grammar} library. The algorithm was trained on two different grammars: on the PCFG which was generated by our program and on the golden standart grammar which was implemented with the help of standard nltk tools. As ain input viterbi algorithm takes one of the grammars and a set of sentences for parsing. As an output viterbi gives probability of the parsed sentence. As a measure of difference between two PCFGs we take the average difference of probabilities for all sentences in test corpus: \begin{math} average_difference = frac{\sum \limits_{i=1}^n \lvert probablitiyGS_i - probabilityPred_i \rvert}{n} \end{math}  where  \begin{math} probabillityGS_i \end{math} is a golden standard probability for i-th sentence and \begin{math} probabillityPred_i \end{math} is a probability which was counted based on grammar provided by our PCFG generator , n is amount of sentences in test corpora.

\subsection{Restrictions}
The verification algorithm has a couple of restrictions. 

Firstly, due to big volume of processed data algorithm works for a rather long time. Then we probably has no possibility to train our PCFG generator on all senetnces which are provided by penntreebank.tagged-sents(). So we restricted training corpora and focused on examples where the training corpus for our generated PCFG and the test corpus are the same. Those results are not meaningless, since the PCFG generator introduces probabilistic splits which didn't necessarily exist in any one sentence of the training corpus, and this would guarantee that our generated PCFG could parse the whole test corpus. 

Secondly, due to the algorithm doesn't produce rules for giving POS tags to words we can't give as an input the sentence itself. Omitting the pre-terminal -> terminal rules (i.e. the POS tag -> word rules) makes the POS tags the new terminals, with no enforced pre-terminal set. So instead of sentence "Tom went home" for example we produce sentence "NNP VBD NN" as an input for viterbi parser.
For our generated PCFG, the program already does this by ignoring the words in the training corpus sentences and only using the pre-tagged POS tags. 
For the gold standard grammar, since the treebank parse trees already include the words, we need to strip out those words by removing every "leaf" of the parse trees (every terminal symbol). It's done in two steps: after generating grammar from treebank.parsed-sents() we found all the rules which contains terminals in right-hand side and remove this rules. After doing this, we take the non-terminal which used to be in left-hand side of the removed rules and  then replace that nonterminal symbol in the right-hand sides of the remaining rules with an equivalent string-type symbol.

\subsection{Result table}

We've trained golden standard on 200 sentences from Penn treebank and tried to parse first 5 sentences. Then we changed amount of sentences in training corpus for our PCFG generator. The results that we have are:

\begin{table}[ht]
\caption{Probability Difference Results}
\centering
\begin{tabular}{c c}
\hline\hline
Amount of training sentences & Average probability difference \\ [0.5ex]
%heading
\hline
10&2.29780078167e-09 \\
11&5.96846840278e-09 \\
12&2.29632689706e-11 \\
13&9.80549819091e-11 \\
14& 2.16508358289e-13 \\
15&4.78059521477e-14 \\
16&2.03665914123e-11 \\
17&4.90643909047e-12 \\
18&3.08436807231e-13 \\
19&6.70629030797e-13 \\
30&2.62390640066e-18 \\ [1ex]
\hline
\end{tabular}
\label{table:nonlin}
\end{table}


\subsection{Potential Improvements}

This implementation of a PCFG generation program could be improves significantly in several ways. At the moment, all valid 2-gram expansions and rule joinings which exists in the training corpus are executed, and the generation process only terminates once all such modifications have been completed. Allowing for the possibility of valid but statistically-negligible modification would improve the system's behavior on large training sets. Similarly, it is very possible for a generated grammar to include groups of production rules which could be simplified down to an equivalent form with fewer rules (as can be seen in the above example). This would limit the growth of the overall size of the generated PCFG's rule set, and might even create additional valid rule joinings of which the generation program could make use.

Another major improvement would be the incorporation of word and sentences probabilities into the training data. If the generation program could be run on raw sentences rather than on POS-tag sequences, and could account for the relative probabilities of each POS mapping to each word, then the generated grammar would be able to parse raw sentences directly, which would improve the accuracy of its results and massively increase its ability to generate natural-sounding random sentences. Similarly, incorporating information about the relative frequency of the training corpus's sentences would prevent less-grammatical input data from distorting the overall structure of the generated PCFG.

%----------------------------------------------------------------------------------------
%	Part 6
%----------------------------------------------------------------------------------------
\section{Conclusion}

Here will be a conclusion

\end{document}

