%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{enumerate} % Allows customizing enumeration blocks

\usepackage{qtree} % Render parse trees

\usepackage[noend]{algpseudocode}
\usepackage{algorithm} % create pseudocode blocks

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)


%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{MIT, 6.863 Natural Language Processing and Computer Representation of Knowledge}
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Automatic PCFG Generator \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author {Todd Layton, Anastasia Uryasheva}

\date{\normalsize\today} 

\begin{document}

\maketitle % 

%----------------------------------------------------------------------------------------
%	Part 1
%----------------------------------------------------------------------------------------

\section{Introduction}
This project is dedicated to the process of deriving PCFG rules from corpus of sentences. Although there are some different articles that are dedicated to the process of deriving grammar rules from large corpora very few of them are about PCFG. Mostly that works are about CFG rules. The difference between CFG and PCFG is a parameter \begin{math} q(\alpha \rightarrow \beta) \end{math} for each rule \begin{math} \alpha\rightarrow \beta \in R \end{math}. The parameter \begin{math} q(\alpha \rightarrow \beta) \end{math} can be interpreted as the conditional probabilty of choosing rule \begin{math} \alpha \rightarrow \beta \end{math} in a left-most derivation, given that the non-terminal being expanded is \begin{math} \alpha \end{math}. For any \begin{math} X \in N \end{math}, we have the constraint \begin{math} \sum_{\alpha\rightarrow\beta\in R:\alpha=X} q(\alpha\rightarrow\beta) = 1 \end{math}. In addition we have \begin{math} q(\alpha\rightarrow\beta) \geqslant 0 \end{math} for any \begin{math} \alpha \rightarrow \beta \in R \end{math}. 
That's why probably there are much less articles about deriving PCFG than about deriving CFG. Those who'd like to derive CFG from corpus has a precise goal: to minimize amount of rules in grammar. In contrast with CFG, PCFG has no precise measure of quality - the less is not better here. Moreover, the maximization of probability for each rule is not a good measure too since it's rather simple to create a dummy grammar with 1.0 probability for each rule. 
Some algorithms for deriving PCFG were prorposed. Some of them rely on different heuristics to iteratively construct an approximation of the unknown CFG, like [1]; others search for a PCFG that has the largest posterior given the training corpus, like [2]. Article [3] describes algorithm which use iterative biclustering. All of them have different restrictions or limitations. Often articles for the automatic PCFG generation uses some additional tool for creating PCFG like [4] that uses LFG approximations, or [5] that uses head-driven structure grammars, or [6] that describes simply representation of linguistics trees in the form of PCFG. There are more articles about creating CFGs using different ways, as for example [7] or [8]. As a theoretical backgorund for our research we took article [9] about deriving CFG and made some significant changes.


%----------------------------------------------------------------------------------------
%	Part 2
%----------------------------------------------------------------------------------------

\section{Background}

The theoretical background of the project is based on the article "Automatic Learning of Context-Free Grammar" written by T.Chen, C.Tseng, C.Chen. The article describes the problem of learning a (non-probabilistic) context-free grammar from a corpus. A solution to this problem is investigated based on the notion of minimizing the description length of the corpus. 
The basic problem is to find a set of production rules, which will together can derive each of the original sentences. The article an iterative approach to create a CFG from a corpus of sentences, and suggests a cost function as a measure of the quality of the resulting CFG. With the help of cost function, the goal is rewritten as: to find a set of rules that can derive the original language with the minimum cost.

\subsection{Cost function}

The cost function measures the `effectiveness' of the generated CFG's description of the corpus. There are two types of cost functions mentioned in this article:

\begin{enumerate}[1.]
\item Rules for giving words their POS tags

For rules consisting of a non-terminal symbol on the left-hand side and a string of symbols on the right-hand side the cost of a rule is the number of bits needed to represent the left-hand side and right-hand side. For example, if we have the rule:
\begin{equation}\label{first}
A  -> \beta
\end{equation}
then the cost function for this rule will be:
\begin{equation}\label{first}
C_R = (1+\lvert\beta\rvert)log(\lvert\Sigma\rvert)
\end{equation}
where \begin{math} \Sigma \end{math}  is the symbol set and \begin{math} \lvert\Sigma\rvert \end{math} is the number of symbols in \begin{math} \Sigma \end{math} .
\item Rules for deriving sentences.

To derive sentence W we need a sequence of rules: \begin{math} ROOT \Rightarrow \alpha_1 \Rightarrow ... \Rightarrow W \end{math}. This sequence of rules always starts with one of S-derivation rules: \begin{math} S \Rightarrow \alpha \end{math}. This step results in a derived string \begin{math} \alpha \end{math}. If there is no non-terminal symbols in \begin{math} \alpha \end{math} , we are done with the derivation. Otherwise, we expand the left-most non-terminal symbol, say X, in \begin{math} \alpha \end{math} by one of its derivation bodies. The process continues until there is no non-terminal symbols in the derived string, which will be the sentence W at that point.
So if we have a set of rules: R1(ROOT) : $ROOT \rightarrow ... , ... , R1(X) : X \rightarrow ... , ...$ and a sentence for derivation W then the cost for rules to derive W will be:
\begin{equation}\label{first}
C_D = \sum \limits_{k=1}^m \log(\lvert R(s_k)\rvert)
\end{equation}
where $m$ is the number of rules in the derivation sequence, \begin{math} s_k \end{math} is the non-terminal symbol for the $k$th derivation and \begin{math} \lvert R(s_k)\rvert \end{math} is the number of rules in the CFG using \begin{math} s_k \end{math} as the left-hand side.
\end{enumerate}

Combining (2.2) and (2.3), the total cost is
\begin{equation}\label{first}
C = \sum \limits_{i=1}^p C_R(i) + \sum\limits_{j=1}^q C_D(j) = \sum \limits_{i=1}^p n_i \log(\lvert \Sigma \rvert) + \sum\limits_{j=1}^q \sum\limits_{k=1}^{m_j} \log(\lvert R(s_k) \rvert)
\end{equation}

\subsection{Special Cases}
Additionally, the article investigates two special case grammars:
\begin{enumerate}[1.]
\item Exhaustive CFG

An exhaustive CFG is one that uses every distinct sentence in the corpus as a direct derivation body of the start symbol (in our case \texttt{ROOT}). The number of symbols for a rule is simply the number of words of the corresponding sentence \begin{math} n_w \end{math}, plus $1$ (for the start symbol), and \begin{math} \lvert \Sigma \rvert \end{math} is the vocabulary size \begin{math} \lvert V \rvert \end{math} of the corpus plus 1 (for the start symbol). The rule cost is:
\begin{equation}\label{first}
C_R = n \log(\lvert \Sigma \rvert) = (n_w + 1) \log(\lvert V \rvert + 1)
\end{equation}
In this case, each sentence is derived from ROOT in one step, by specifying the correct one out of the \begin{math} \lvert R(ROOT) \rvert \end{math} rules. Thus the derivation cost for a sentence is:
\begin{equation}\label{first}
C_D = \log \lvert R(ROOT) \rvert
\end{equation}
 Total cost for that case is:

\begin{equation}\label{first}
C = \sum \limits_{i=1}^{\lvert R(ROOT) \rvert} C_R(i) + \sum \limits_{j=1}^q C_D(j) = \sum \limits_{i=1}^{\lvert R(ROOT) \rvert} (n_w(i) + 1) log(\lvert V \rvert + 1) + q log \lvert R(ROOT) \rvert
\end{equation}

\item Recursive CFG

A recursive CFG is one that uses recursive derivation for \texttt{ROOT}; \texttt{ROOT \textrightarrow A ROOT} where the non-terminal \texttt{A} can be expanded to be any terminal in the vocabulary.
The rule cost in this case is:

\begin{equation}\label{first}
C_R = n \log \lvert \Sigma \rvert
\end{equation}
where n can be 1, 2 or 3 depending on the rule.
The derivation cost in this case is:

\begin{equation}\label{first}
C_D = n_w(1 + \log \lvert V \rvert) + 1
\end{equation}

Total cost in this case:

\begin{equation}\label{first}
C = \sum \limits_{i=1}^{2+\lvert V \rvert}n_i log \lvert \Sigma \rvert + \sum \limits_{j=1}^q C_D(j) = (4 + 2\lvert V \rvert)log(\lvert V \rvert + 2) + \sum \limits_{j=1}^q (n_w(j)(1 + log \lvert V \rvert) + 1)
\end{equation}

\end{enumerate}

The exhaustive CFG is too restricted in the sense that it covers only those sentences seen in the learning corpus. The recursive CFG is too broad in the sense that it covers all sentences including the non-sense ones. The goal is to strike a balance between these two extremes.

\subsection{Incorporating Probabilities}
Although the general theoretic background of this article is sound, our algorithm for the automatic generation of PCFGs has several significant differences, resulting from the incorporation of probabilistic parameters into the grammar's rules. The primary difference is that, while the approach described in the article seeks only to minimize the size of the generated grammar, without attempting to expand the language defined by that grammar, our generation program aims to produce a grammar which defines a language that is larger than just the sentences in the training corpus, even if this expansion comes at the expense of grammar size. For that reason, we use a quality measure different from the cost functions described above.

Since the generation process introduces non-terminal symbols with arbitrary names into the corpus, and does not attempt to collapse redundancies in the grammar as it is generated, we cannot directly compare the parse trees created by the generated grammar to gold standard parse trees. Instead, we use the average difference between the parse probabilities of each sentence, as parsed by the generated PCFG and by a gold standard PCFG respectively. The goal is to minimize this average difference, on the assumption that the parse probability of a sentence is roughly equivalent to its grammaticality. Within this measure, a higher average probability is generally better, though it is important to note that it is trivial to make a PCFG which always gives a parse of probability 1. Thus, the abstract objective of the generation program is to produce a PCFG which assigns to each sentence a parse probability as close as possible to that given by the gold standard grammar.


%----------------------------------------------------------------------------------------
%	Part 3
%----------------------------------------------------------------------------------------

\section{Design}

This program uses a POS-tagged training corpus to generate a PCFG, through an iterative process of production rule creation and modification by transforming an initial trivial grammar into a more generalized one.

\subsection{Simplifying Assumptions}

This PCFG generator program make a couple basic assumptions to reduce the complexity of the problem, allowing it to focus more effectively on the core of the automated generation concept.

\subsubsection{Pre-Tagged Training Corpus}

Like many statistical parsers, the PCFGs generated by this program only describe production rules down to the level of parts of speech; the grammars' terminal symbols are not specific words, but rather POS tags. This means that the important training input to the generator is not a set of sentences in themselves, but a set of POS-tag sequences corresponding to a set of sentences. While this could be achieved by incorporating a separate tagging preparation step into the program itself, we chose to instead reduce the scope of the generator to exclude tagging; instead, the program's training corpus input is a set of POS-tagged sentences, which can have been tagged in any way the user sees fit.

\subsubsection{Flat-Probability Training Corpus}

The program calculates the relative frequency of various features in the grammar, as it is generated, on the assumption that each of the tagged sentences in the training corpus has the same flat probability in the source language (namely, a probability of 1 over the size-in-sentences of the corpus). While this reduces the amount of input data necessary, it also removed the possibility of the generation process taking into account the true relative probabilities of the training corpus's content, which could have a non-negligible impact on the resulting PCFG when training on larger corpora.

\subsection{Initial Grammar}

The program uses the input training corpus to create a exhaustive grammar as a starting point for the rule modification. For each POS-tagged sentence in the training corpus, a corresponding production rule is added to the grammar. The left-hand side of this rule is \texttt{ROOT}, the right-hand side is the sequence of POS tags constituting that sentence, and the probability is $1$ over the total size, in sentences, of the corpus.
\subsection{Iteration}

As described above, exhaustive grammars are maximally specific to their generating corpus, so once the initial grammar is created, the program repeatedly looks for modifications to make in order to produce a grammar which is not so closely fitted to the training corpus. Generation of the PCFG is complete once there are no remaining modifications to be made. These modifications consist of two types of grammar transformations: 2-gram expansion and rule joining.

\subsubsection{2-gram Expansion}

The program finds the most frequently-occurring symbol 2-gram in the training corpus which is not already the right-hand side of a production rule in the grammar. A new nonterminal symbol is created, specified as an expansion of that 2-gram's symbols and with a unique numeric identifier, and each instance of that 2-gram is replaced with that nonterminal. An additional production rule is added to the grammar, of which the left-hand side is the new nonterminal, the right-hand side is the 2-gram, and the probability is $1$. This step does not change the functional structure of the grammar, but rather decomposed it into a larger number of simpler rules in order to enable rule joining.

\subsubsection{Rule Joining}

Before each 2-gram expansion step, the program checks whether any multiple production rules in the corpus, with more than one symbol in the right-hand side, consists of the exact same right-hand side except for the symbols at one particular index (for example, the third symbol in each rule's right-hand side). If such rules are found, then a new nonterminal symbol is created, specified as a rule joining of the symbols being replaced and with a unique numeric identifier, and the symbol at that index in each rule's right-hand side is replaced with that nonterminal. Then, for each of those rules, a new rule is added to the grammar, of which the left-hand side is the new nonterminal, the right-hand side is the symbol which that nonterminal replaced in that rule, and the probability is the relative frequency of the right-hand side symbol. Unlike 2-gram expansion, rule joining actually changes the functional structure of the grammar, allowing for symbol sequence which did not previously fit.

%----------------------------------------------------------------------------------------
%	Part 4
%----------------------------------------------------------------------------------------

\section{Implementation}

The \texttt{pcfggen} module's primary access point is the \texttt{generate\_pcfg()} function.

\subsection{Control Flow}

\begin{algorithm}
\caption{Pseudocode program flow of \texttt{pcfggen} module's \texttt{generate\_pcfg()} function}
\begin{algorithmic}
\Function{Generate-PCFG}{$S$} \Comment $S$: training corpus of POS-tagged sentences
\State $G = \text{Initial-Grammar($S$)}$
\Loop
\While{$G$ has valid rule joining $J$}
\State $G = \text{Join-Rules($G$, $J$)}$
\EndWhile
\If{$G$ has valid most-frequent 2-gram expansion $E$}
\State $G = \text{Expand-2-Gram($G$, $E$)}$
\Else
\State \Return $G$
\EndIf
\EndLoop
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Example}

The iterative results of this function are best illustrated with a simple example. Consider a toy corpus of three sentences, corresponding to three POS tag sequences:

\begin{enumerate}[(a)]
\item \texttt{A C B C}
\item \texttt{X A C}
\item \texttt{Y B C}
\end{enumerate}

Given this corpus as the training input, the program would perform the following steps:
\begin{enumerate}[1.]
\item Create the initial grammar:
\begin{enumerate}[(a)]
\item \texttt{ROOT \textrightarrow A C B C [1/3]}
\item \texttt{ROOT \textrightarrow X A C [1/3]}
\item \texttt{ROOT \textrightarrow B C Y [1/3]}
\end{enumerate}

\item Check for rule joining, but no production rules are sufficiently similar.

\item Expand the 2-gram \texttt{A C}:
\begin{enumerate}[(a)]
\item \texttt{ROOT \textrightarrow exp(A)(C)-0 B C [1/3]}
\item \texttt{ROOT \textrightarrow X exp(A)(C)-0 [1/3]}
\item \texttt{ROOT \textrightarrow B C Y [1/3]}
\item \texttt{exp(A)(C)-0 \textrightarrow A C  [1]}
\end{enumerate}

\item Check for rule joining, but no production rules are sufficiently similar.

\item Expand the 2-gram \texttt{B C}:
\begin{enumerate}[(a)]
\item \texttt{ROOT \textrightarrow exp(A)(C)-0 exp(B)(C)-1 [1/3]}
\item \texttt{ROOT \textrightarrow X exp(A)(C)-0 [1/3]}
\item \texttt{ROOT \textrightarrow exp(B)(C)-0 Y [1/3]}
\item \texttt{exp(A)(C) \textrightarrow A C  [1]}
\item \texttt{exp(B)(C) \textrightarrow B C  [1]}
\end{enumerate}

\item Join \texttt{A C} and \texttt{B C}:
\begin{enumerate}[(a)]
\item \texttt{ROOT \textrightarrow exp(A)(C)-0 exp(B)(C)-1 [1/3]}
\item \texttt{ROOT \textrightarrow X exp(A)(C)-0 [1/3]}
\item \texttt{ROOT \textrightarrow exp(B)(C)-1 Y [1/3]}
\item \texttt{exp(A)(C)-0 \textrightarrow join(A)(B)-2 C  [1]}
\item \texttt{exp(B)(C)-1 \textrightarrow join(A)(B)-2 C  [1]}
\item \texttt{join(A)(B)-2 \textrightarrow A  [1/2]}
\item \texttt{join(A)(B)-2 \textrightarrow B  [1/2]}
\end{enumerate}

\item Check for rule joining, but no production rules are sufficiently similar.

\item Check for 2-gram expansion, but no unexpanded 2-grams remain.

\item Return the grammar.

\end{enumerate}

The resulting generated PCFG is able to parse not only the training sentences, but also other sentences of a similar structure. For example, the sentence \texttt{X B C} would be parsed as follows:

\Tree [.{ROOT[ $1 \over 3$]} X [.{exp(A)(C)-0[$1$]} [.{join(A)(B)-2[$1 \over 2$]} B ] C ]]

Similarly, the generated grammar is capable of randomly producing sentences which were not present in the training corpus. \texttt{X B C} is a possible random-sentence output of this PCFG, with a probability of ${1 \over 3} \cdot 1 \cdot {1 \over 2} = {1 \over 6}$.

%----------------------------------------------------------------------------------------
%	Part 5
%----------------------------------------------------------------------------------------

\section{Results}

To check the quality of generated PCFG we decided to parse every sentence by \texttt{viterbi parser} which is a part of \texttt{nltk.grammar} library. The algorithm was trained on two different grammars: on the PCFG which was generated by our program and on the golden standart grammar which was implemented with the help of standard nltk tools. As ain input viterbi algorithm takes one of the grammars and a set of sentences for parsing. As an output viterbi gives probability of the parsed sentence. As a measure of difference between two PCFGs we take the average difference of probabilities for all sentences in test corpus: \begin{math} average_difference = frac{\sum \limits_{i=1}^n \lvert probablitiyGS_i - probabilityPred_i \rvert}{n} \end{math}  where  \begin{math} probabillityGS_i \end{math} is a golden standard probability for i-th sentence and \begin{math} probabillityPred_i \end{math} is a probability which was counted based on grammar provided by our PCFG generator , n is amount of sentences in test corpora.

\subsection{Restrictions}
The verification algorithm has a couple of restrictions. 

Firstly, due to big volume of processed data algorithm works for a rather long time. Then we probably has no possibility to train our PCFG generator on all senetnces which are provided by penntreebank.tagged-sents(). So we restricted training corpora and focused on examples where the training corpus for our generated PCFG and the test corpus are the same. Those results are not meaningless, since the PCFG generator introduces probabilistic splits which didn't necessarily exist in any one sentence of the training corpus, and this would guarantee that our generated PCFG could parse the whole test corpus. 

Secondly, due to the algorithm doesn't produce rules for giving POS tags to words we can't give as an input the sentence itself. Omitting the pre-terminal -> terminal rules (i.e. the POS tag -> word rules) makes the POS tags the new terminals, with no enforced pre-terminal set. So instead of sentence "Tom went home" for example we produce sentence "NNP VBD NN" as an input for viterbi parser.
For our generated PCFG, the program already does this by ignoring the words in the training corpus sentences and only using the pre-tagged POS tags. 
For the gold standard grammar, since the treebank parse trees already include the words, we need to strip out those words by removing every "leaf" of the parse trees (every terminal symbol). It's done in two steps: after generating grammar from treebank.parsed-sents() we found all the rules which contains terminals in right-hand side and remove this rules. After doing this, we take the non-terminal which used to be in left-hand side of the removed rules and  then replace that nonterminal symbol in the right-hand sides of the remaining rules with an equivalent string-type symbol.

\subsection{Result table}

We've trained golden standard on 200 sentences from Penn treebank and tried to parse first 5 sentences. Then we changed amount of sentences in training corpus for our PCFG generator. The results that we have are:

\begin{table}[ht]
\caption{Probability Difference Results}
\centering
\begin{tabular}{c c}
\hline\hline
Amount of training sentences & Average probability difference \\ [0.5ex]
%heading
\hline
10&2.29780078167e-09 \\
11&5.96846840278e-09 \\
12&2.29632689706e-11 \\
13&9.80549819091e-11 \\
14& 2.16508358289e-13 \\
15&4.78059521477e-14 \\
16&2.03665914123e-11 \\
17&4.90643909047e-12 \\
18&3.08436807231e-13 \\
19&6.70629030797e-13 \\
30&2.62390640066e-18 \\ [1ex]
\hline
\end{tabular}
\label{table:nonlin}
\end{table}


\subsection{Potential Improvements}

This implementation of a PCFG generation program could be improves significantly in several ways. At the moment, all valid 2-gram expansions and rule joinings which exists in the training corpus are executed, and the generation process only terminates once all such modifications have been completed. Allowing for the possibility of valid but statistically-negligible modification would improve the system's behavior on large training sets. Similarly, it is very possible for a generated grammar to include groups of production rules which could be simplified down to an equivalent form with fewer rules (as can be seen in the above example). This would limit the growth of the overall size of the generated PCFG's rule set, and might even create additional valid rule joinings of which the generation program could make use.

Another major improvement would be the incorporation of word and sentences probabilities into the training data. If the generation program could be run on raw sentences rather than on POS-tag sequences, and could account for the relative probabilities of each POS mapping to each word, then the generated grammar would be able to parse raw sentences directly, which would improve the accuracy of its results and massively increase its ability to generate natural-sounding random sentences. Similarly, incorporating information about the relative frequency of the training corpus's sentences would prevent less-grammatical input data from distorting the overall structure of the generated PCFG.

%----------------------------------------------------------------------------------------
%	Part 6
%----------------------------------------------------------------------------------------
\section{Conclusion}

We have implemented a program to automatically generate a probabilistic context free grammar from a corpus of POS-tagged training sentences. Though this implementation is severely limited by runtime constraints, it demonstrates the conceptual soundness this iterative approach. In particular, the fact that the language defined by the generated grammar contains sentences beyond those found in the training corpus shows how, in theory, an automated program could, using only a sample set of a natural language and a vocabulary dictionary, learn the grammatical structure of that language in its entirety. While this experimental \texttt{pcfggen} module is far from a universal language interpreter, the underlying principles are those necessary to develop a true artificial language learning system.

%----------------------------------------------------------------------------------------
%	Part 7
%----------------------------------------------------------------------------------------
\section{References}
\begin{enumerate}[1. ]
\item Adriaans, P., Trautwein, M., Vervoort, M.: Towards high speed grammar induction on large text corpora.
\item Chen, S.F.: Bayesian grammar induction for language modeling.
\item Tu K., Honavar V.: Unsupervised Learning of Probabilistic Context-Free Grammar using Iterative Biclustering.
\item Cahill A., Josef van Genabith: Robust PCFG-Based Generation using Automatically Acquired LFG Approximations.
\item Zhang Y., Krieger H.U. :Large-Scale Corpus-Driven PCFG Approximation of an HPSG.
\item Johnson M. :PCFG Models of Linguistic Tree Representations.
\item Avinash J. Agrawal, Dr. O. G. Kakde, Shri Ramdeobaba, Katol Road: Corpus Based Context Free Grammar Design for Natural Language Interface to Database.
\item Post M., GildeaBayesian D. :Learning of a Tree Substitution Grammar.
\item T.Chen, C.Tseng, C.Chen: Automatic Learning of Context-Free Grammar.
\end{enumerate}

\end{document}

